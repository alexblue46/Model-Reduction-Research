import tensorflow as tf
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

"""
TensorFlowExample.py

Creates a neural network using TensorFlow to predict the class/color of a 
data point given its x and y coordinates. The training data is generated by 
scikit-learn's make_moons function. The neural network consists of two input 
nodes (the data's x and y coordinates) and two output nodes (the probability 
the data point belongs to either class). The number and sizes of the hidden
layers can be altered to see its effect on the neural network's performance.
"""

def generate_data(num, noise):
    """
    Inputs:
        num - number of training data
        noise - noise of the training data
        
    Generates the training data.
    """
    np.random.seed(0)
    X, y = datasets.make_moons(num, noise=noise)
    return X, y

def plot_data(X, y, shade):
    """
    Inputs:
        X - (x,y) coordinates of data to plot
        y - boolean array indicating color of each data point
        shade - float between 0 and 1 to darken the color of the data points
        
    Plots the data.
    """
    my_cmap = plt.cm.Spectral(np.arange(plt.cm.Spectral.N))
    my_cmap[:,0:3] *= shade
    my_cmap = ListedColormap(my_cmap)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)
    
def plot_decision_boundary(pred_func, X):
    """
    Inputs:
        X - training data
        pred_func - lambda that predicts the class of data
        
    Plots the decision boundaries the neural network created for the different 
    classes.
    """
    x_min, x_max = X[:, 0].min() - 0.25, X[:, 0].max() + 0.25
    y_min, y_max = X[:, 1].min() - 0.25, X[:, 1].max() + 0.25
    h = 0.01
    x_vals = np.arange(x_min, x_max, h)
    y_vals = np.arange(y_min, y_max, h)
    # Takes the cartesian product of x_vals and y_vals
    mesh = np.transpose([np.tile(x_vals, len(y_vals)), 
                            np.repeat(y_vals, len(x_vals))])
    y = np.argmin(pred_func(mesh), axis=1)
    plot_data(mesh, y, 1.0)
    
 
def initialize_NN(hidden_layers):
    """
    Inputs:
        hidden_layers - array of the number of nodes in each of the neural network's 
                        hidden layers
                        
    Initializes the weights and biases of the neural network.
    """
    # Adds 2 to the beginning and end of hidden layers to account for the input 
    # nodes and output nodes
    layers = hidden_layers
    layers.insert(0,2)
    layers.append(2)
    weights = []
    biases = []
    num_layers = len(layers) 
    for l in range(num_layers-1):
        W = xavier_init(size=[layers[l], layers[l+1]])
        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float64), 
                        dtype=tf.float64)
        weights.append(W)
        biases.append(b)
    return weights, biases
    
def xavier_init(size):
    """
    Inputs:
        size - array of size two, indicating the number of nodes in the current
               layer and the following layer
               
    Helper function for initialize_NN. Creates the variable for the weights at
    the given layer.
    """
    in_dim = size[0]
    out_dim = size[1]
    xavier_stddev = np.sqrt(in_dim)
    return tf.Variable(tf.random_normal([in_dim, out_dim], stddev=xavier_stddev, 
                                        dtype=tf.float64), dtype=tf.float64)    
    
def forward_prop(X, weights, biases):
    """
    Inputs:
        X - array of values of input nodes
        weights - array of the weights at each layer in the neural network
        biases - array of the biases at each layer in the neural network
        
    Computes the forward propagation of the neural network and returns the 
    values at the outputs nodes.
    """
    H = X
    for layer in range(len(weights) - 1):
        W = tf.Print(weights[layer], [weights[layer]], "W: ")
        b = tf.Print(biases[layer], [biases[layer]], "B: ")
        # Activation function is tanh
        H = tf.tanh(tf.add(tf.matmul(H, W), b))
    W = weights[-1]
    b = biases[-1]
    Y = tf.add(tf.matmul(H, W), b)
    return Y

    
class NN:
    """
    Class to model the neural network
    """
    
    def __init__(self, X, y, hidden_layers, print_loss):
        """
        Inputs:
            X - training data's (x,y) coordinates
            y - training data's classifications
            hidden_layers - array of the number of nodes in each of the neural 
                            network's hidden layers
            print_loss - boolean indicating whether the loss of the neural network
                         should be displayed every thousand iterations of training
                         
        Initializes the neural network. Defines the loss function of the neural 
        network and the algorihtm that TensorFlow will use to train the neural
        network.
        """
        self.print_loss = print_loss
        
        self.X = X
        self.y = np.stack((y, 1*np.logical_not(y))).transpose()
        self.weights, self.biases = initialize_NN(hidden_layers)
        
        self.y_tf = tf.placeholder(tf.float64, shape=[None, 2])
        self.X_tf = tf.placeholder(tf.float64, shape=[None, 2])
        self.sol_predict = self.run_NN(self.X_tf)
        self.loss = tf.losses.softmax_cross_entropy(self.y_tf, self.sol_predict)
        
        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(self.loss)
        
        self.sess = tf.Session()
        init = tf.global_variables_initializer()
        self.sess.run(init)
        
    def run_NN(self, X):
        """
        Inputs:
            X - array of values of input nodes
            
        Runs the forward propagation of the classes neural network with the 
        given input nodes and returns the values of the output nodes.
        """
        return forward_prop(X, self.weights, self.biases)
        
    def sol_train(self, N_iter):
        """
        Inputs:
            N_iter - number of iterations that will be used in training
            
        Trains the neural network with the class's training data. Displays the
        loss of the neural network every thousand iterations of training if 
        print_loss is set to True. 
        """
        tf_dict = {self.X_tf: self.X, self.y_tf: self.y}
        for iter in range(N_iter + 1):
            if (self.print_loss and iter % 1000 == 0):
                loss_value = self.sess.run(self.loss, tf_dict)
                print("Loss after iteration %i: %f" % (iter, loss_value))
            self.sess.run(self.optimizer, tf_dict)
            
    def predict_sol(self, X_predict):
        """
        Inputs:
            X_predict - (x,y) coordinates of the data to be predicted
            
        Predicts the classes of the data using the neural network.
        """
        return self.sess.run(self.sol_predict, {self.X_tf : X_predict})
                

def create_and_eval(num_training, training_noise, training_iter, hidden_layers, print_loss):
    """
    Inputs:
        num_training - number of training data
        training_noise - noise of the training data
        training_iter - number of iterations that will be used in training
        hidden_layers - array of the number of nodes in each of the neural network's 
                        hidden layers
        print_loss - boolean indicating whether the loss of the neural network
                     should be displayed every thousand iterations of training
                     
    Generates training data and trains the neural network with this data. Plots 
    the training data with the decision boundary for the neural network.                
    """
    print "Number of training data:", num_training
    print "Noise of training data:", training_noise
    print "Number of training iterations:", training_iter
    print "Hidden layers:", hidden_layers
    print "\n"
    
    X,y = generate_data(num_training, training_noise)
    model = NN(X, y, hidden_layers, print_loss)
    model.sol_train(training_iter)
    plot_decision_boundary(lambda mesh: model.predict_sol(mesh), X)
    plot_data(X, y, 0.8)
    plt.title("Decision Boundary for Neural Network")

    
create_and_eval(10, 0.2, 20000, [5], True)